# Neural_Network_from_Scratch

This project was created to explore how a neural network works, including its various components, layers, and functions. This neural network was written from scratch, and does not utilize the machine learning framworks Tensorflow or Pytorch. Instead, the libraries Numpy, Pandas, and Random were imported for matrices, data manipulation, and number generation support.

Overview
---------
In recent years, neural networks have become a deep area of interest and research due to their immense application potential. Similar to the human brain, neural networks are implemented through "neurons" that help them learn and model relationships, making it the backbone of machine learning. In essence, it is a series of algorithms that aim to recognize relationships in a set of data.

Components
---------
Generally, all neural networks contain some variation of these implementations:
 - Input layer: Where the network receives data. Each input neuron represents a single feature or attribute of the data.
 - Hidden layer(s): Where the network performs computations and transfers information from the input nodes to the output nodes. Some networks contain many hidden layers, which can be classified as "deep" neural networks.
 - Output layer: Where the final prediction is made or classification is made.
 - Weights and Biases: The parameters that the network adapts through learning. Weights control the degree to which two neurons are connected, which can become weaker or stronger. 
 - Activation Functions: These determine the output of a neuron. Generally, there are two common types of activation functions, ReLU and Sigmoid.
